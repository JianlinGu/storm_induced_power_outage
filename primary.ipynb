{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc0f93de-9758-44e4-b07d-dd5aad7ffa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Script for Feature Engineering and Deriving Master Dataset(storms_data)\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.neighbors import BallTree\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "120de3c0-65e4-4eb2-93f8-3c7c48a3f4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path().resolve()\n",
    "\n",
    "DATA_RAW = BASE_DIR / \"data\" / \"raw\"\n",
    "\n",
    "# County list\n",
    "ne_coastal = pd.read_csv(\n",
    "    DATA_RAW / \"northeast_counties\" / \"ne_coastal_counties_fips.csv\"\n",
    ")\n",
    "# NOAA Storm Events\n",
    "storms_data = pd.read_csv(\n",
    "    DATA_RAW / \"storm_events\" / \"StormEvents_2014_2022_NE_coastal.csv\"\n",
    ")\n",
    "\n",
    "# EAGLE-I power outages\n",
    "outage_df = pd.read_csv(\n",
    "    DATA_RAW / \"power_outages\" / \"eaglei_outages_2014_2022_NE_coastal_raw.csv\"\n",
    ")\n",
    "\n",
    "# Housing units\n",
    "hu20202024 = pd.read_csv(\n",
    "    DATA_RAW / \"housing_units\" / \"to_process_hu2020-2024.csv\"\n",
    ")\n",
    "\n",
    "hu20102020 = pd.read_csv(\n",
    "    DATA_RAW / \"housing_units\" / \"to_process_hu2010-2020.csv\",\n",
    "    encoding=\"latin1\",\n",
    ")\n",
    "\n",
    "# ERA5 storm intensity (directory)\n",
    "ERA5_DIR = DATA_RAW / \"storm_intensity\" / \"era5_NE_coastal_county_hourly\"\n",
    "\n",
    "# US road network / density\n",
    "roads_file = DATA_RAW / \"road_density\" / \"raw\"\n",
    "\n",
    "COUNTY_SHP = (\n",
    "    DATA_RAW\n",
    "    / \"road_density\"\n",
    "    / \"data_county_boundary\"\n",
    "    / \"cb_2018_us_county_500k.shp\"\n",
    ")\n",
    "\n",
    "counties_shape = gpd.read_file(COUNTY_SHP)\n",
    "\n",
    "# EIA 861 (distribution circuits)\n",
    "dist_sys = pd.read_excel(\n",
    "    DATA_RAW / \"circuit_distribution\" / \"Distribution_Systems_2023.xlsx\"\n",
    ")\n",
    "serv_terr = pd.read_excel(\n",
    "    DATA_RAW / \"circuit_distribution\" / \"Service_Territory_2023.xlsx\"\n",
    ")\n",
    "# RUCC code\n",
    "rucc = pd.read_csv(\n",
    "    DATA_RAW / \"rucc\" / \"Ruralurbancontinuumcodes2023.csv\",\n",
    "    encoding=\"latin1\",\n",
    ")\n",
    "\n",
    "# County Business Patterns (CBP)\n",
    "cbp = pd.read_csv(\n",
    "    DATA_RAW / \"county_business_pattern\" / \"merged_coastal_counties_data.csv\"\n",
    ")\n",
    "# Urban area shapefile\n",
    "gdf_urban = (\n",
    "    gpd.read_file(\n",
    "        DATA_RAW\n",
    "        / \"shapefiles\"\n",
    "        / \"national urban area shapefile\"\n",
    "        / \"tl_2020_us_uac20.shp\"\n",
    "    )\n",
    "    .to_crs(\"EPSG:4326\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4040d6be-66ef-439f-8da4-23cdd2d18327",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['LOCATION_INDEX', 'RANGE',\n",
    "       'AZIMUTH', 'LOCATION', 'LAT2', 'LON2','EVENT_TYPE','INJURIES_DIRECT',\n",
    "       'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT',\n",
    "       'DAMAGE_PROPERTY', 'DAMAGE_CROPS', 'MAGNITUDE', 'MAGNITUDE_TYPE','BEGIN_RANGE', 'BEGIN_AZIMUTH', 'END_RANGE',\n",
    "       'END_AZIMUTH', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON','key', 'STATE_UP', 'WFO', 'FLOOD_CAUSE', 'CZ_NAME_UP', 'CZ_TIMEZONE', 'EPISODE_ID_DET']\n",
    "\n",
    "drop_counties = {\n",
    "    \"NEW LONDON\",\n",
    "    \"FAIRFIELD\",\n",
    "    \"MIDDLESEX\",\n",
    "    \"NEW HAVEN\",\n",
    "}\n",
    "\n",
    "mask = (\n",
    "    storms_data[\"STATE\"].eq(\"CONNECTICUT\") &\n",
    "    storms_data[\"CZ_NAME\"].isin(drop_counties)\n",
    ")\n",
    "\n",
    "storms_data = storms_data.loc[~mask].copy()\n",
    "storms_data = storms_data.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1511a829-d491-481e-8661-39f72875a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from storm_outage_after24h import match_max_outage_after24h\n",
    "\n",
    "storms_data = match_max_outage_after24h(\n",
    "    storm_df = storms_data,\n",
    "    outage_df = outage_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26e1d0f5-5b33-4226-9694-3d345322ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from era5_storm_features_max48h import run_all_stream\n",
    "storms_data = run_all_stream(storms_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b95da4-c3bd-492e-8b39-e8c90c742b9a",
   "metadata": {},
   "source": [
    "Housing_units_datasets Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0e83558-5175-4135-9f0d-608d3848ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from housing_units_process import housing_units_process\n",
    "hu = housing_units_process(hu20202024, hu20102020, ne_coastal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0070af6-daf5-457c-94f9-799c07708302",
   "metadata": {},
   "outputs": [],
   "source": [
    "hu[\"fips_str\"] = hu[\"fips_str\"].astype(str).str.zfill(5)\n",
    "year_cols = [c for c in hu.columns if str(c).isdigit() and 2010 <= int(c) <= 2024]\n",
    "\n",
    "hu_long = (\n",
    "    hu.melt(\n",
    "        id_vars=[\"fips_str\"],\n",
    "        value_vars=year_cols,\n",
    "        var_name=\"year\",\n",
    "        value_name=\"housing_units\",\n",
    "    )\n",
    ")\n",
    "\n",
    "hu_long[\"year\"] = hu_long[\"year\"].astype(int)\n",
    "hu_long[\"housing_units\"] = pd.to_numeric(hu_long[\"housing_units\"], errors=\"coerce\")\n",
    "storms_data[\"STATE_FIPS\"] = storms_data[\"STATE_FIPS\"].astype(str).str.zfill(2)\n",
    "storms_data[\"CZ_FIPS\"]    = storms_data[\"CZ_FIPS\"].astype(str).str.zfill(3)\n",
    "storms_data[\"fips_str\"]   = storms_data[\"STATE_FIPS\"] + storms_data[\"CZ_FIPS\"]\n",
    "\n",
    "storms_data[\"YEAR\"] = pd.to_numeric(storms_data[\"YEAR\"], errors=\"coerce\").astype(\"Int64\")  # 允许缺失\n",
    "storms_data = storms_data.merge(\n",
    "    hu_long.rename(columns={\"year\": \"YEAR\"}),\n",
    "    on=[\"fips_str\", \"YEAR\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "storms_data[\"max_outage_after_24h\"] = pd.to_numeric(storms_data[\"max_outage_after_24h\"], errors=\"coerce\")\n",
    "storms_data[\"outage_ratio\"] = np.where(\n",
    "    (storms_data[\"housing_units\"].notna()) & (storms_data[\"housing_units\"] > 0),\n",
    "    storms_data[\"max_outage_after_24h\"] / storms_data[\"housing_units\"],\n",
    "    np.nan\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa42d43-2654-4e25-b0d6-374594de3743",
   "metadata": {},
   "source": [
    "Road_datasets Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12ae9870-c547-40be-ae63-c6a415fc1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from road_datasets_process import road_datasets_process\n",
    "road_density = road_datasets_process(roads_file, ne_coastal, counties_shape)\n",
    "\n",
    "rd = road_density.copy()\n",
    "rd[\"fips_str\"] = rd[\"county_fips\"].astype(str).str.zfill(5)\n",
    "rd = rd[[\"fips_str\", \"road_density_km_per_km2\"]]\n",
    "storms_data[\"fips_str\"] = (storms_data[\"STATE_FIPS\"].astype(str).str.zfill(2)+ storms_data[\"CZ_FIPS\"].astype(str).str.zfill(3))\n",
    "storms_data = storms_data.merge(rd,on=\"fips_str\",how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257afb6-4e28-4b7d-9c3b-841dec1ad804",
   "metadata": {},
   "source": [
    "Circuits_by_road_density Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c745db7c-df1c-46fa-9b3d-7608d98f799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  county_fips State      County  circuits_total\n",
      "0       09001    CT   Fairfield      352.809755\n",
      "1       09007    CT   Middlesex       93.953592\n",
      "2       09009    CT   New Haven      352.156725\n",
      "3       09011    CT  New London      136.459881\n",
      "4       23005    ME  Cumberland       55.569240\n",
      "(44, 4)\n"
     ]
    }
   ],
   "source": [
    "from circuits_distribution_process import circuits_distribution_process\n",
    "storms_data = circuits_distribution_process(dist_sys, serv_terr, road_density, storms_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a3632-bba3-4b48-b36c-6938613e2c99",
   "metadata": {},
   "source": [
    "RUCC Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "706bd8b2-f3f5-4e94-aef8-664a9c2c407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rucc[\"fips_str\"] = rucc[\"FIPS\"].astype(str).str.zfill(5)\n",
    "rucc_rucc = (rucc.loc[rucc[\"Attribute\"] == \"RUCC_2023\", [\"fips_str\", \"Value\"]].rename(columns={\"Value\": \"rucc_2023\"}))\n",
    "rucc_rucc[\"rucc_2023\"] = pd.to_numeric(rucc_rucc[\"rucc_2023\"], errors=\"coerce\")\n",
    "\n",
    "storms_data[\"STATE_FIPS\"] = storms_data[\"STATE_FIPS\"].astype(str).str.zfill(2)\n",
    "storms_data[\"CZ_FIPS\"]    = storms_data[\"CZ_FIPS\"].astype(str).str.zfill(3)\n",
    "storms_data[\"fips_str\"]   = storms_data[\"STATE_FIPS\"] + storms_data[\"CZ_FIPS\"]\n",
    "\n",
    "storms_data = storms_data.merge(rucc_rucc,on=\"fips_str\",how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f08208b-8b32-4b23-a033-6a78c10f86f0",
   "metadata": {},
   "source": [
    "Season_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61e85537-7fb7-4dca-9200-0d381284792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_data[\"season\"] = storms_data['MONTH_NAME'].map({\n",
    "    'December': \"winter\", 'January': \"winter\", 'February': \"winter\",\n",
    "    'March': \"spring\", 'April': \"spring\", 'May': \"spring\",\n",
    "    'June': \"summer\", 'July': \"summer\", 'August': \"summer\",\n",
    "    'September': \"fall\", 'October': \"fall\", 'November': \"fall\",\n",
    "})\n",
    "storms_data[\"season\"] = storms_data[\"season\"].astype(\"category\")\n",
    "storms_data[\"season_code\"] = storms_data[\"season\"].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76ebfb-0af4-46b6-8f2d-9ef4b86a6547",
   "metadata": {},
   "source": [
    "UG_ratio_proxy UG_ratio_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39a99594-9c56-48f3-abe6-6c9205ab7507",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = storms_data[\"urban_score\"].to_numpy(dtype=float)\n",
    "r = storms_data[\"rd_norm\"].to_numpy(dtype=float)\n",
    "C = storms_data[\"weighted_number_of_circuits\"].to_numpy(dtype=float)\n",
    "y = storms_data[\"outage_ratio\"].to_numpy(dtype=float)\n",
    "\n",
    "mask = np.isfinite(u) & np.isfinite(r) & np.isfinite(C) & np.isfinite(y)\n",
    "u, r, C, y = u[mask], r[mask], C[mask], y[mask]\n",
    "\n",
    "best_a, best_b = None, None\n",
    "best_err = np.inf\n",
    "best_rho = None\n",
    "\n",
    "for a in range(0, 5):\n",
    "    for b in range(0, 5):\n",
    "        ug = expit(a * u + b * r)\n",
    "        pred = (1 - ug) * C\n",
    "\n",
    "        rho, _ = spearmanr(y, pred)\n",
    "        if np.isnan(rho):\n",
    "            continue\n",
    "\n",
    "        err = 1 - rho   # 越小越好\n",
    "\n",
    "        if err < best_err:\n",
    "            best_err = err\n",
    "            best_a, best_b = a, b\n",
    "            best_rho = rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62e31cd0-67db-466f-b48f-82050bf7a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_data[\"UG_ratio_proxy\"] = expit(\n",
    "    3 * storms_data[\"urban_score\"]\n",
    "  + 2 * storms_data[\"rd_norm\"]\n",
    ")\n",
    "\n",
    "storms_data[\"overhead_circuits\"] = (\n",
    "    1 - storms_data[\"UG_ratio_proxy\"]\n",
    ") * storms_data[\"weighted_number_of_circuits\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3518cc23-0235-4127-b789-ed8388263084",
   "metadata": {},
   "source": [
    "county business pattern process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d81d609-b457-47f6-93f8-032728601394",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbp[\"fipstate\"] = cbp[\"fipstate\"].astype(str).str.zfill(2)\n",
    "cbp[\"fipscty\"]  = cbp[\"fipscty\"].astype(str).str.zfill(3)\n",
    "cbp[\"full_fips\"] = cbp[\"fipstate\"] + cbp[\"fipscty\"]\n",
    "\n",
    "cbp = cbp[~cbp[\"full_fips\"].str.startswith(\"09\")].copy()\n",
    "\n",
    "cbp[\"emp\"] = pd.to_numeric(cbp[\"emp\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "cbp_county = (\n",
    "    cbp.groupby(\"full_fips\", as_index=False)[\"emp\"]\n",
    "       .sum()\n",
    "       .rename(columns={\"emp\": \"cbp_emp_total\"})\n",
    ")\n",
    "\n",
    "storms_data[\"full_fips\"] = storms_data[\"full_fips\"].astype(str).str.zfill(5)\n",
    "\n",
    "storms_data = storms_data.merge(\n",
    "    cbp_county,\n",
    "    on=\"full_fips\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55b6c6bb-95c5-4090-97be-f08c6a0bf273",
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_data = storms_data[storms_data[\"YEARMONTH\"] >= 201501].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbe6fa09-b516-44ad-bf89-c4e05c5cd1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "era_cols = [\n",
    "    \"era_i10fg_max_total_48h\",\n",
    "    \"era_tp_max_total_48h\",\n",
    "    \"era_crr_max_total_48h\",\n",
    "]\n",
    "\n",
    "storms_data[era_cols] = (\n",
    "    storms_data\n",
    "    .groupby(\"full_fips\")[era_cols]\n",
    "    .transform(lambda x: x.fillna(x.median()))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b680050-da5b-40ea-947c-38a14204f706",
   "metadata": {},
   "source": [
    "point overlap for the small counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7dd1d-2c80-4850-b78d-a1c513635d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from small_county_ERA5_overlap import small_county_ERA5_overlap\n",
    "storms_data = small_county_ERA5_overlap(ERA5_DIR, storms_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d705e0f8-522a-4fdb-82fa-21db13f1025a",
   "metadata": {},
   "source": [
    "npoints, nurban and urban ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09b93fcf-dfee-4f98-8167-df5f5f921a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strom_impact_location_exposure import strom_impact_location_exposure\n",
    "storms_data = strom_impact_location_exposure(gdf_urban, storms_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a1b01404-8f31-4e87-b05b-9ec00fd364df",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTIES_SHP = r\"F:\\Storm Outage Modeling\\data\\raw\\shapefiles\\ne_counties\\NE_coastal_counties.shp\"\n",
    "gdf = gpd.read_file(COUNTIES_SHP)\n",
    "gdf[\"CZ_FIPS\"] = gdf[\"GEOID\"].astype(int)\n",
    "gdf[\"area_km2\"] = gdf[\"ALAND\"] / 1e6\n",
    "county_area_df = gdf[[\"CZ_FIPS\", \"area_km2\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8530c02-7925-4444-9ca9-0cf97cb950fd",
   "metadata": {},
   "source": [
    "baseline construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c972db-2308-4bb3-8c67-46ec87ee7dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_outage_construction import baseline_outage_construction\n",
    "storms_data = baseline_outage_construction(storms_data, outage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b932670c-afba-424f-9d2c-7b07613d28f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "storms = storms_data[[\"CZ_FIPS\", \"BEGIN_DATE_TIME\", \"END_DATE_TIME\"]].copy()\n",
    "storms[\"BEGIN_DATE_TIME\"] = pd.to_datetime(storms[\"BEGIN_DATE_TIME\"])\n",
    "storms[\"END_DATE_TIME\"] = pd.to_datetime(storms[\"END_DATE_TIME\"])\n",
    "storms[\"CZ_FIPS\"] = storms[\"CZ_FIPS\"].astype(int)\n",
    "\n",
    "outage = outage_df[[\"fips_code\", \"run_start_time\", \"sum\"]].copy()\n",
    "outage = outage.rename(columns={\"fips_code\": \"CZ_FIPS\"})\n",
    "outage[\"CZ_FIPS\"] = outage[\"CZ_FIPS\"].astype(int)\n",
    "outage[\"run_start_time\"] = pd.to_datetime(outage[\"run_start_time\"])\n",
    "\n",
    "\n",
    "def compute_baseline_for_county(df_out, df_storm):\n",
    "    if df_storm.empty:\n",
    "        return df_out[\"sum\"].median()\n",
    "\n",
    "    t = df_out[\"run_start_time\"].values\n",
    "    is_storm = np.zeros(len(df_out), dtype=bool)\n",
    "\n",
    "    for _, r in df_storm.iterrows():\n",
    "        is_storm |= (t >= r[\"BEGIN_DATE_TIME\"]) & (t <= r[\"END_DATE_TIME\"])\n",
    "\n",
    "    baseline_vals = df_out.loc[~is_storm, \"sum\"]\n",
    "\n",
    "    if baseline_vals.empty:\n",
    "        return np.nan\n",
    "\n",
    "    return baseline_vals.median()\n",
    "\n",
    "baseline_records = []\n",
    "\n",
    "for cz, df_out_c in outage.groupby(\"CZ_FIPS\"):\n",
    "    df_storm_c = storms.loc[storms[\"CZ_FIPS\"] == cz]\n",
    "    baseline = compute_baseline_for_county(df_out_c, df_storm_c)\n",
    "\n",
    "    baseline_records.append(\n",
    "        {\n",
    "            \"CZ_FIPS\": cz,\n",
    "            \"baseline_outage_median\": baseline,\n",
    "        }\n",
    "    )\n",
    "baseline_df = pd.DataFrame(baseline_records)\n",
    "baseline_df = baseline_df.copy()\n",
    "baseline_df.columns = baseline_df.columns.str.strip()\n",
    "\n",
    "storms_data = storms_data.copy()\n",
    "storms_data.columns = storms_data.columns.str.strip()\n",
    "\n",
    "baseline_df[\"fips5\"] = baseline_df[\"CZ_FIPS\"].astype(str).str.zfill(5)\n",
    "\n",
    "candidate_cols = [c for c in baseline_df.columns if c not in {\"CZ_FIPS\", \"fips5\"}]\n",
    "\n",
    "\n",
    "preferred = [c for c in candidate_cols if \"baseline\" in c.lower() or \"median\" in c.lower()]\n",
    "baseline_col = preferred[0] if preferred else candidate_cols[0]\n",
    "\n",
    "\n",
    "if \"full_fips\" in storms_data.columns:\n",
    "    storms_data[\"fips5\"] = pd.to_numeric(storms_data[\"full_fips\"], errors=\"coerce\").astype(\"Int64\").astype(str).str.zfill(5)\n",
    "elif \"fips_str\" in storms_data.columns:\n",
    "    storms_data[\"fips5\"] = storms_data[\"fips_str\"].astype(str).str.extract(r\"(\\d+)\")[0].str.zfill(5)\n",
    "else:\n",
    "    raise KeyError(\"no full_fips or fips_str\")\n",
    "\n",
    "tmp = baseline_df[[\"fips5\", baseline_col]].rename(columns={baseline_col: \"baseline_outage_median\"})\n",
    "\n",
    "if \"baseline_outage_median\" in storms_data.columns:\n",
    "    storms_data = storms_data.drop(columns=[\"baseline_outage_median\"])\n",
    "\n",
    "storms_data = storms_data.merge(\n",
    "    tmp,\n",
    "    on=\"fips5\",\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a9e0b3a-cb80-4a9f-b63e-ae54430a25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _first_stable_off_time(times: np.ndarray,\n",
    "                           outage: np.ndarray,\n",
    "                           baseline: float,\n",
    "                           start_idx: int,\n",
    "                           stable_steps: int):\n",
    "\n",
    "    if start_idx >= len(outage):\n",
    "        return pd.NaT\n",
    "\n",
    "    if stable_steps <= 1:\n",
    "        j = np.where(outage[start_idx:] <= baseline)[0]\n",
    "        if len(j) == 0:\n",
    "            return pd.NaT\n",
    "        return pd.Timestamp(times[start_idx + j[0]])\n",
    "\n",
    "    ok = (outage <= baseline)\n",
    "    ok2 = ok[start_idx:]\n",
    "\n",
    "    if len(ok2) < stable_steps:\n",
    "        return pd.NaT\n",
    "\n",
    "    sums = np.convolve(ok2.astype(int), np.ones(stable_steps, dtype=int), mode=\"valid\")\n",
    "    k = np.where(sums == stable_steps)[0]\n",
    "    if len(k) == 0:\n",
    "        return pd.NaT\n",
    "\n",
    "    off_idx = start_idx + int(k[0])\n",
    "    return pd.Timestamp(times[off_idx])\n",
    "\n",
    "\n",
    "def add_outage_duration_by_baseline(\n",
    "    storms_data: pd.DataFrame,\n",
    "    outage_df: pd.DataFrame,\n",
    "    baseline_df: pd.DataFrame,\n",
    "    *,\n",
    "    # key / columns\n",
    "    storm_begin_col: str = \"BEGIN_DATE_TIME\",\n",
    "    storm_end_col: str = \"END_DATE_TIME\",\n",
    "    outage_time_col: str = \"run_start_time\",\n",
    "    outage_value_col: str = \"sum\",\n",
    "    outage_fips_col: str = \"fips_code\",\n",
    "    baseline_fips_col: str = \"CZ_FIPS\",\n",
    "    baseline_value_col: str = \"baseline_outage_median\",\n",
    "    # search / stability params\n",
    "    post_hours: int = 72,\n",
    "    pre_hours: int = 0,\n",
    "    stable_steps: int = 4,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    s = storms_data.copy()\n",
    "    o = outage_df.copy()\n",
    "    b = baseline_df.copy()\n",
    "\n",
    "    s.columns = s.columns.str.strip()\n",
    "    o.columns = o.columns.str.strip()\n",
    "    b.columns = b.columns.str.strip()\n",
    "\n",
    "    if \"full_fips\" in s.columns:\n",
    "        s[\"fips5\"] = (\n",
    "            pd.to_numeric(s[\"full_fips\"], errors=\"coerce\")\n",
    "            .astype(\"Int64\")\n",
    "            .astype(str)\n",
    "            .str.zfill(5)\n",
    "        )\n",
    "    elif \"fips_str\" in s.columns:\n",
    "        s[\"fips5\"] = (\n",
    "            s[\"fips_str\"].astype(str).str.extract(r\"(\\d+)\")[0].str.zfill(5)\n",
    "        )\n",
    "    elif \"CZ_FIPS\" in s.columns:\n",
    "        s[\"fips5\"] = (\n",
    "            pd.to_numeric(s[\"CZ_FIPS\"], errors=\"coerce\")\n",
    "            .astype(\"Int64\")\n",
    "            .astype(str)\n",
    "            .str.zfill(5)\n",
    "        )\n",
    "    else:\n",
    "        raise KeyError(\"no county FIPS\")\n",
    "\n",
    "    o[outage_time_col] = pd.to_datetime(o[outage_time_col])\n",
    "    o[\"fips5\"] = (\n",
    "        pd.to_numeric(o[outage_fips_col], errors=\"coerce\")\n",
    "        .astype(\"Int64\")\n",
    "        .astype(str)\n",
    "        .str.zfill(5)\n",
    "    )\n",
    "    o[outage_value_col] = pd.to_numeric(o[outage_value_col], errors=\"coerce\")\n",
    "\n",
    "    if baseline_value_col not in b.columns:\n",
    "        raise KeyError(f\"baseline_df 缺少列 {baseline_value_col}\")\n",
    "\n",
    "    b[\"fips5\"] = (\n",
    "        pd.to_numeric(b[baseline_fips_col], errors=\"coerce\")\n",
    "        .astype(\"Int64\")\n",
    "        .astype(str)\n",
    "        .str.zfill(5)\n",
    "    )\n",
    "    if \"baseline_outage_median\" in s.columns:\n",
    "        s = s.drop(columns=[\"baseline_outage_median\"])\n",
    "\n",
    "    s = s.merge(\n",
    "        b[[\"fips5\", baseline_value_col]].rename(columns={baseline_value_col: \"baseline_outage_median\"}),\n",
    "        on=\"fips5\",\n",
    "        how=\"left\",\n",
    "        validate=\"many_to_one\",\n",
    "    )\n",
    "\n",
    "    s[storm_begin_col] = pd.to_datetime(s[storm_begin_col])\n",
    "    s[storm_end_col] = pd.to_datetime(s[storm_end_col])\n",
    "    o = o.dropna(subset=[\"fips5\", outage_time_col]).sort_values([\"fips5\", outage_time_col])\n",
    "    outage_groups = {k: df for k, df in o.groupby(\"fips5\", sort=False)}\n",
    "\n",
    "    pre_delta = pd.Timedelta(hours=pre_hours)\n",
    "    post_delta = pd.Timedelta(hours=post_hours)\n",
    "\n",
    "    t_on_list, t_off_list, dur_hours_list = [], [], []\n",
    "    for _, row in s.iterrows():\n",
    "        fips5 = row[\"fips5\"]\n",
    "        O0 = row[\"baseline_outage_median\"]\n",
    "        t0 = row[storm_begin_col]\n",
    "        t1 = row[storm_end_col]\n",
    "\n",
    "        if pd.isna(O0) or pd.isna(t0) or pd.isna(t1) or (fips5 not in outage_groups):\n",
    "            t_on_list.append(pd.NaT)\n",
    "            t_off_list.append(pd.NaT)\n",
    "            dur_hours_list.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        df_out = outage_groups[fips5]\n",
    "\n",
    "        win_start = t0 - pre_delta\n",
    "        win_end = t1 + post_delta\n",
    "        df_win = df_out.loc[(df_out[outage_time_col] >= win_start) & (df_out[outage_time_col] <= win_end)]\n",
    "\n",
    "        if df_win.empty:\n",
    "            t_on_list.append(pd.NaT)\n",
    "            t_off_list.append(pd.NaT)\n",
    "            dur_hours_list.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        times = df_win[outage_time_col].to_numpy()\n",
    "        vals = df_win[outage_value_col].to_numpy()\n",
    "\n",
    "        # t_on\n",
    "        on = np.where(vals > float(O0))[0]\n",
    "        if len(on) == 0:\n",
    "            t_on_list.append(pd.NaT)\n",
    "            t_off_list.append(pd.NaT)\n",
    "            dur_hours_list.append(0.0)\n",
    "            continue\n",
    "\n",
    "        on_idx = int(on[0])\n",
    "        t_on = pd.Timestamp(times[on_idx])\n",
    "\n",
    "        # t_off\n",
    "        t_off = _first_stable_off_time(\n",
    "            times=times,\n",
    "            outage=vals,\n",
    "            baseline=float(O0),\n",
    "            start_idx=on_idx + 1,\n",
    "            stable_steps=stable_steps,\n",
    "        )\n",
    "\n",
    "        if pd.isna(t_off):\n",
    "            duration_hours = np.nan\n",
    "        else:\n",
    "            duration_hours = (t_off - t_on) / pd.Timedelta(hours=1)\n",
    "\n",
    "        t_on_list.append(t_on)\n",
    "        t_off_list.append(t_off)\n",
    "        dur_hours_list.append(duration_hours)\n",
    "\n",
    "    s[\"t_on\"] = t_on_list\n",
    "    s[\"t_off\"] = t_off_list\n",
    "    s[\"duration_hours\"] = dur_hours_list\n",
    "\n",
    "    return s\n",
    "\n",
    "storms_data = add_outage_duration_by_baseline(storms_data, outage_df, baseline_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e8e6a5-4c73-4b09-82d1-b9fe86deb539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "counties = ne_coastal.copy()\n",
    "print(counties.crs)\n",
    "counties_eq = counties.to_crs(\"EPSG:5070\") \n",
    "counties_eq[\"county_area_km2\"] = counties_eq.geometry.area / 1e6\n",
    "area_df = (\n",
    "    counties_eq[[\"GEOID\", \"county_area_km2\"]]\n",
    "    .rename(columns={\"GEOID\": \"CZ_FIPS\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4ae45063-ba58-446b-88a0-5cf64ebd5fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_data['housing_units_by_area'] = storms_data['housing_units']/storms_data['county_area_km2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a187e575-8ae0-4b7a-8846-2b566a4e8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_data['cbp_emp_total'] = storms_data['cbp_emp_total']/storms_data['county_area_km2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "24018f64-9003-49b6-9764-60bf70d0c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_data.to_csv(r'F:\\Storm Outage Modeling\\storms_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad0a431-c302-4b30-b65a-f447eb5d2ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
