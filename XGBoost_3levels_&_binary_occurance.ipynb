{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62b8fdcb-cc89-471e-a9ef-37609f402946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal XGBoost, Threshold models, Three severity levels\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.special import expit\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "BASE_DIR = Path().resolve()\n",
    "\n",
    "CSV_PATH = BASE_DIR / \"storms_data.csv\"\n",
    "storms_data = pd.read_csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "728c7681-a5b7-43af-9127-8ae49bbc34a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Info] sev_ratio distribution:\n",
      "count    9667.000000\n",
      "mean        0.020446\n",
      "std         0.056287\n",
      "min         0.000000\n",
      "50%         0.006146\n",
      "75%         0.015572\n",
      "90%         0.047092\n",
      "95%         0.087721\n",
      "99%         0.250419\n",
      "max         0.997414\n",
      "Name: sev_ratio, dtype: float64\n",
      "\n",
      "[Check] y4 class proportions (%):\n",
      "y4\n",
      "0    44.01\n",
      "1    36.03\n",
      "2    10.47\n",
      "3     9.50\n",
      "\n",
      "[Check] merged y (target) class proportions (%):\n",
      "y\n",
      "0    44.01\n",
      "1    46.50\n",
      "2     9.50\n",
      "\n",
      "[Check] cumulative rates:\n",
      "P(y>=1) train=0.560, test=0.560\n",
      "P(y>=2) train=0.095, test=0.095\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9438    0.9471    0.9455       851\n",
      "           1     0.9509    0.9488    0.9499       899\n",
      "           2     0.9727    0.9674    0.9700       184\n",
      "\n",
      "    accuracy                         0.9498      1934\n",
      "   macro avg     0.9558    0.9544    0.9551      1934\n",
      "weighted avg     0.9499    0.9498    0.9499      1934\n",
      "\n",
      "[[806  42   3]\n",
      " [ 44 853   2]\n",
      " [  4   2 178]]\n",
      "AUC P(y>=1): 0.9912\n",
      "AUC P(y>=2): 0.9970\n",
      "\n",
      "===== Feature Importance (Average over thresholds) =====\n",
      "\n",
      "housing_units_by_area      0.136918\n",
      "cbp_emp_total              0.135906\n",
      "n_points                   0.131775\n",
      "n_urban                    0.122300\n",
      "era_i10fg_max_total_48h    0.108519\n",
      "era_crr_max_total_48h      0.098886\n",
      "era_tp_max_total_48h       0.098020\n",
      "season_code                0.088472\n",
      "overhead_circuits          0.079202\n",
      "\n",
      "===== Feature Importance (Per threshold) =====\n",
      "\n",
      "                          P(y>=1)   P(y>=2)\n",
      "cbp_emp_total            0.112522  0.159291\n",
      "housing_units_by_area    0.129012  0.144824\n",
      "era_i10fg_max_total_48h  0.094171  0.122867\n",
      "n_points                 0.145767  0.117784\n",
      "season_code              0.071314  0.105630\n",
      "era_crr_max_total_48h    0.092365  0.105407\n",
      "era_tp_max_total_48h     0.094447  0.101593\n",
      "n_urban                  0.170331  0.074270\n",
      "overhead_circuits        0.090072  0.068333\n"
     ]
    }
   ],
   "source": [
    "#Column configuration (MATCH YOUR CSV)\n",
    "OUTAGE_COL = \"max_outage_after_24h\"\n",
    "BASELINE_COL = \"baseline_outage_median\"\n",
    "HU_COL = \"housing_units\"\n",
    "\n",
    "\n",
    "# Feature configuration (edit if needed)\n",
    "feature_cols = [\n",
    "    \"era_i10fg_max_total_48h\",\n",
    "    \"era_tp_max_total_48h\",\n",
    "    \"era_crr_max_total_48h\",\n",
    "    'housing_units_by_area',\n",
    "    \"overhead_circuits\",\n",
    "    \"n_points\",\n",
    "    \"n_urban\",\n",
    "    \"season_code\",\n",
    "    \"cbp_emp_total\",\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# 3) Sanity checks: required columns exist\n",
    "# -------------------------\n",
    "required_cols = list(set(feature_cols + [OUTAGE_COL, BASELINE_COL, HU_COL]))\n",
    "missing = [c for c in required_cols if c not in storms_data.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing required columns in CSV: {missing}\")\n",
    "\n",
    "# -------------------------\n",
    "# 4) Config\n",
    "# -------------------------\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "USE_EXCESS = True  # True: (max_outage - baseline), False: use max_outage\n",
    "\n",
    "# Engineering thresholds (4-level specification)\n",
    "# Level 0: <0.5% ; Level 1: [0.5%,2%) ; Level 2: [2%,5%) ; Level 3: >=5%\n",
    "CUTS = [0.005, 0.02, 0.05]\n",
    "\n",
    "# XGBoost parameters (safe defaults)\n",
    "XGB_PARAMS = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    n_estimators=800,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Build sev_ratio and ordinal labels\n",
    "# -------------------------\n",
    "df0 = storms_data.copy()\n",
    "\n",
    "max_out = pd.to_numeric(df0[OUTAGE_COL], errors=\"coerce\")\n",
    "base = pd.to_numeric(df0[BASELINE_COL], errors=\"coerce\")\n",
    "hu = pd.to_numeric(df0[HU_COL], errors=\"coerce\").replace(0, np.nan)\n",
    "\n",
    "z = (max_out - base) if USE_EXCESS else max_out\n",
    "df0[\"sev_ratio\"] = (z / hu).clip(lower=0)  # truncate negative noise to 0\n",
    "\n",
    "# Convert features to numeric where possible (keep NaN if non-parsable)\n",
    "for c in feature_cols:\n",
    "    df0[c] = pd.to_numeric(df0[c], errors=\"coerce\")\n",
    "\n",
    "df = df0.dropna(subset=feature_cols + [\"sev_ratio\"]).copy()\n",
    "\n",
    "print(\"\\n[Info] sev_ratio distribution:\")\n",
    "print(df[\"sev_ratio\"].describe(percentiles=[0.5, 0.75, 0.9, 0.95, 0.99]))\n",
    "\n",
    "def ratio_to_level_4(x: float, cuts=CUTS) -> int:\n",
    "    if x < cuts[0]:\n",
    "        return 0\n",
    "    elif x < cuts[1]:\n",
    "        return 1\n",
    "    elif x < cuts[2]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "df[\"y4\"] = df[\"sev_ratio\"].apply(ratio_to_level_4).astype(int)\n",
    "\n",
    "print(\"\\n[Check] y4 class proportions (%):\")\n",
    "print((df[\"y4\"].value_counts(normalize=True).sort_index().mul(100).round(2)).to_string())\n",
    "\n",
    "if df[\"y4\"].nunique() < 2:\n",
    "    raise ValueError(\"y4 has <2 classes under current CUTS; adjust thresholds.\")\n",
    "\n",
    "# ---- Merge Level 1 and Level 2 ----\n",
    "# original: 0,1,2,3  -> merged: 0,1,2 where (1,2)->1 and 3->2\n",
    "df[\"y\"] = df[\"y4\"].replace({2: 1, 3: 2}).astype(int)\n",
    "\n",
    "print(\"\\n[Check] merged y (target) class proportions (%):\")\n",
    "print((df[\"y\"].value_counts(normalize=True).sort_index().mul(100).round(2)).to_string())\n",
    "\n",
    "levels = sorted(df[\"y\"].unique().tolist())\n",
    "if levels != [0, 1, 2]:\n",
    "    raise ValueError(f\"Expected merged y to have levels [0,1,2], got {levels}\")\n",
    "\n",
    "# Train/test split (shared for all thresholds)\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df[\"y\"].astype(int).copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "# For K=3 ordinal levels, train 2 cumulative models: P(y>=1), P(y>=2)\n",
    "y_ge_1_train = (y_train >= 1).astype(int)\n",
    "y_ge_2_train = (y_train >= 2).astype(int)\n",
    "\n",
    "y_ge_1_test = (y_test >= 1).astype(int)\n",
    "y_ge_2_test = (y_test >= 2).astype(int)\n",
    "\n",
    "print(\"\\n[Check] cumulative rates:\")\n",
    "print(f\"P(y>=1) train={y_ge_1_train.mean():.3f}, test={y_ge_1_test.mean():.3f}\")\n",
    "print(f\"P(y>=2) train={y_ge_2_train.mean():.3f}, test={y_ge_2_test.mean():.3f}\")\n",
    "\n",
    "# Train cumulative binary XGB models\n",
    "\n",
    "m_ge_1 = xgb.XGBClassifier(**XGB_PARAMS)\n",
    "m_ge_2 = xgb.XGBClassifier(**XGB_PARAMS)\n",
    "\n",
    "m_ge_1.fit(X_train, y_ge_1_train)\n",
    "m_ge_2.fit(X_train, y_ge_2_train)\n",
    "\n",
    "# Predict and enforce monotonicity\n",
    "\n",
    "p_ge_1 = m_ge_1.predict_proba(X_test)[:, 1]\n",
    "p_ge_2 = m_ge_2.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Enforce monotonicity: P(y>=1) >= P(y>=2)\n",
    "p_ge_2 = np.minimum(p_ge_2, p_ge_1)\n",
    "\n",
    "# Convert to 3-class probabilities:\n",
    "P0 = 1.0 - p_ge_1\n",
    "P1 = p_ge_1 - p_ge_2\n",
    "P2 = p_ge_2\n",
    "\n",
    "proba_ord = np.vstack([P0, P1, P2]).T  # (n,3)\n",
    "y_pred = np.argmax(proba_ord, axis=1)\n",
    "\n",
    "#Reports\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "try:\n",
    "    print(f\"AUC P(y>=1): {roc_auc_score(y_ge_1_test, p_ge_1):.4f}\")\n",
    "except ValueError as e:\n",
    "    print(\"AUC P(y>=1) not available (likely only one class in y_ge_1_test).\", e)\n",
    "\n",
    "try:\n",
    "    print(f\"AUC P(y>=2): {roc_auc_score(y_ge_2_test, p_ge_2):.4f}\")\n",
    "except ValueError as e:\n",
    "    print(\"AUC P(y>=2) not available (likely only one class in y_ge_2_test).\", e)\n",
    "\n",
    "\n",
    "# Feature importance (per-threshold + average)\n",
    "imp1 = pd.Series(m_ge_1.feature_importances_, index=feature_cols, name=\"P(y>=1)\")\n",
    "imp2 = pd.Series(m_ge_2.feature_importances_, index=feature_cols, name=\"P(y>=2)\")\n",
    "\n",
    "imp_table = pd.concat([imp1, imp2], axis=1)\n",
    "imp_avg = imp_table.mean(axis=1).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n===== Feature Importance (Average over thresholds) =====\\n\")\n",
    "print(imp_avg.to_string())\n",
    "\n",
    "print(\"\\n===== Feature Importance (Per threshold) =====\\n\")\n",
    "print(imp_table.sort_values(by=\"P(y>=2)\", ascending=False).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d77ffaf-bbc8-4f6f-b908-089ae4886372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.991\n",
      "AP  = 0.999\n",
      "\n",
      "===== Classification Report =====\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.84      0.91       129\n",
      "           1       0.99      1.00      1.00      2296\n",
      "\n",
      "    accuracy                           0.99      2425\n",
      "   macro avg       0.99      0.92      0.95      2425\n",
      "weighted avg       0.99      0.99      0.99      2425\n",
      "\n",
      "\n",
      "===== Confusion Matrix =====\n",
      "\n",
      "[[ 109   20]\n",
      " [   2 2294]]\n"
     ]
    }
   ],
   "source": [
    "threshold = 10\n",
    "\n",
    "storms_data[\"y_outage\"] = (\n",
    "    storms_data[\"max_outage_after_24h\"]\n",
    "    > storms_data[\"baseline_outage_median\"] + threshold\n",
    ").astype(int)\n",
    "\n",
    "feature_cols = [\n",
    "    # Hazard (storm intensity)\n",
    "    \"era_i10fg_max_total_48h\",\n",
    "    \"era_tp_max_total_48h\",\n",
    "    \"era_crr_max_total_48h\",\n",
    "\n",
    "    # Exposure\n",
    "    'housing_units_by_area',\n",
    "    'overhead_circuits',\n",
    "\n",
    "    # Fragility / resilience\n",
    "    'n_points', 'n_urban',\n",
    "\n",
    "    # Context\n",
    "    \"season_code\",'cbp_emp_total'\n",
    "]\n",
    "\n",
    "\n",
    "df = storms_data.dropna(subset=feature_cols + [\"y_outage\"]).copy()\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[\"y_outage\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    n_estimators=500,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "p_test = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (p_test >= 0.5).astype(int)   # 或者用 model.predict(X_test)\n",
    "\n",
    "auc = roc_auc_score(y_test, p_test)\n",
    "ap = average_precision_score(y_test, p_test)\n",
    "\n",
    "print(f\"AUC = {auc:.3f}\")\n",
    "print(f\"AP  = {ap:.3f}\")\n",
    "\n",
    "print(\"\\n===== Classification Report =====\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\n===== Confusion Matrix =====\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
