{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a48adf5-fa87-4236-b0a7-ef43692a47aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Network Baseline (Naive Bayes BN) for Ordinal Severity (3-class merged)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "from pgmpy.models import BayesianNetwork, DiscreteBayesianNetwork\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.inference import VariableElimination\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path().resolve()\n",
    "\n",
    "CSV_PATH = BASE_DIR / \"storms_data.csv\"\n",
    "storms_data = pd.read_csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b4ad6a-fb50-4937-9e82-12a907dacdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pgmpy: Datatype (N=numerical, C=Categorical Unordered, O=Categorical Ordered) inferred from data: \n",
      " {'era_i10fg_max_total_48h': 'N', 'era_tp_max_total_48h': 'N', 'era_crr_max_total_48h': 'N', 'housing_units_by_area': 'N', 'overhead_circuits': 'N', 'n_points': 'N', 'n_urban': 'N', 'season_code': 'N', 'cbp_emp_total': 'N', 'y': 'N'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Check] merged y class proportions (%):\n",
      "y\n",
      "0    44.01\n",
      "1    46.50\n",
      "2     9.50\n",
      "\n",
      "===== BN Naive Bayes (3-class) : Classification Report =====\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5547    0.5006    0.5263       851\n",
      "           1     0.5456    0.6051    0.5738       899\n",
      "           2     0.1834    0.1685    0.1756       184\n",
      "\n",
      "    accuracy                         0.5176      1934\n",
      "   macro avg     0.4279    0.4247    0.4252      1934\n",
      "weighted avg     0.5152    0.5176    0.5150      1934\n",
      "\n",
      "\n",
      "===== BN Naive Bayes (3-class) : Confusion Matrix =====\n",
      "\n",
      "[[426 362  63]\n",
      " [280 544  75]\n",
      " [ 62  91  31]]\n",
      "\n",
      "===== Threshold-level ROC AUC (BN) =====\n",
      "AUC P(y>=1): 0.6408\n",
      "AUC P(y>=2): 0.7348\n"
     ]
    }
   ],
   "source": [
    "# Columns\n",
    "OUTAGE_COL = \"max_outage_after_24h\"\n",
    "BASELINE_COL = \"baseline_outage_median\"\n",
    "HU_COL = \"housing_units\"\n",
    "\n",
    "feature_cols = [\n",
    "    \"era_i10fg_max_total_48h\",\n",
    "    \"era_tp_max_total_48h\",\n",
    "    \"era_crr_max_total_48h\",\n",
    "    'housing_units_by_area',\n",
    "    \"overhead_circuits\",\n",
    "    \"n_points\",\n",
    "    \"n_urban\",\n",
    "    \"season_code\",\n",
    "    \"cbp_emp_total\",\n",
    "]\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "USE_EXCESS = True\n",
    "CUTS = [0.005, 0.02, 0.05]   # 4-level cuts, then merge\n",
    "N_BINS = 5                  # discretization bins per numeric feature (tune 4–8)\n",
    "\n",
    "\n",
    "required_cols = list(set(feature_cols + [OUTAGE_COL, BASELINE_COL, HU_COL]))\n",
    "missing = [c for c in required_cols if c not in storms_data.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing required columns in CSV: {missing}\")\n",
    "\n",
    "\n",
    "df0 = storms_data.copy()\n",
    "\n",
    "max_out = pd.to_numeric(df0[OUTAGE_COL], errors=\"coerce\")\n",
    "base = pd.to_numeric(df0[BASELINE_COL], errors=\"coerce\")\n",
    "hu = pd.to_numeric(df0[HU_COL], errors=\"coerce\").replace(0, np.nan)\n",
    "\n",
    "z = (max_out - base) if USE_EXCESS else max_out\n",
    "df0[\"sev_ratio\"] = (z / hu).clip(lower=0)\n",
    "\n",
    "# numeric coercion\n",
    "for c in feature_cols:\n",
    "    df0[c] = pd.to_numeric(df0[c], errors=\"coerce\")\n",
    "\n",
    "df = df0.dropna(subset=feature_cols + [\"sev_ratio\"]).copy()\n",
    "\n",
    "def ratio_to_level_4(x: float, cuts=CUTS) -> int:\n",
    "    if x < cuts[0]:\n",
    "        return 0\n",
    "    elif x < cuts[1]:\n",
    "        return 1\n",
    "    elif x < cuts[2]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "df[\"y4\"] = df[\"sev_ratio\"].apply(ratio_to_level_4).astype(int)\n",
    "df[\"y\"] = df[\"y4\"].replace({2: 1, 3: 2}).astype(int)  # merged into {0,1,2}\n",
    "\n",
    "levels = sorted(df[\"y\"].unique().tolist())\n",
    "if levels != [0, 1, 2]:\n",
    "    raise ValueError(f\"Expected merged y levels [0,1,2], got {levels}\")\n",
    "\n",
    "print(\"\\n[Check] merged y class proportions (%):\")\n",
    "print((df[\"y\"].value_counts(normalize=True).sort_index().mul(100).round(2)).to_string())\n",
    "\n",
    "\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df[\"y\"].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "train = X_train.copy()\n",
    "train[\"y\"] = y_train.values\n",
    "\n",
    "test = X_test.copy()\n",
    "test[\"y\"] = y_test.values\n",
    "\n",
    "\n",
    "def make_quantile_edges(s: pd.Series, n_bins: int):\n",
    "    \"\"\"Return bin edges based on train quantiles, robust to ties.\"\"\"\n",
    "    s = s.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return None\n",
    "    qs = np.linspace(0, 1, n_bins + 1)\n",
    "    edges = np.quantile(s.values, qs)\n",
    "    edges = np.unique(edges)\n",
    "    if len(edges) < 3:\n",
    "        # fallback: use min/max with small epsilon\n",
    "        mn, mx = float(s.min()), float(s.max())\n",
    "        if mn == mx:\n",
    "            return np.array([mn - 1e-9, mn + 1e-9])\n",
    "        return np.array([mn - 1e-9, mx + 1e-9])\n",
    "    edges[0] = edges[0] - 1e-9\n",
    "    edges[-1] = edges[-1] + 1e-9\n",
    "    return edges\n",
    "\n",
    "def discretize_with_edges(df_in: pd.DataFrame, edges_map: dict):\n",
    "    df_out = df_in.copy()\n",
    "    for col, edges in edges_map.items():\n",
    "        if edges is None:\n",
    "            df_out[col] = 0\n",
    "        else:\n",
    "            df_out[col] = pd.cut(df_out[col], bins=edges, labels=False, include_lowest=True)\n",
    "            df_out[col] = df_out[col].astype(\"float\").fillna(0).astype(int)\n",
    "    return df_out\n",
    "\n",
    "# build edges on train only\n",
    "edges_map = {}\n",
    "for col in feature_cols:\n",
    "    edges_map[col] = make_quantile_edges(train[col], N_BINS)\n",
    "\n",
    "train_disc = discretize_with_edges(train, edges_map)\n",
    "test_disc  = discretize_with_edges(test, edges_map)\n",
    "\n",
    "# ensure y is int\n",
    "train_disc[\"y\"] = train_disc[\"y\"].astype(int)\n",
    "test_disc[\"y\"]  = test_disc[\"y\"].astype(int)\n",
    "\n",
    "\n",
    "edges = [(\"y\", f) for f in feature_cols]\n",
    "model = DiscreteBayesianNetwork(edges)\n",
    "\n",
    "# Fit CPDs with Bayesian (Dirichlet) smoothing\n",
    "model.fit(\n",
    "    train_disc,\n",
    "    estimator=BayesianEstimator,\n",
    "    prior_type=\"BDeu\",\n",
    "    equivalent_sample_size=10,  # smoothing strength (5–50 reasonable)\n",
    ")\n",
    "\n",
    "infer = VariableElimination(model)\n",
    "\n",
    "\n",
    "def predict_proba_bn(infer, df_disc: pd.DataFrame, feature_cols, y_states=(0,1,2)):\n",
    "    probas = np.zeros((len(df_disc), len(y_states)), dtype=float)\n",
    "    for i in range(len(df_disc)):\n",
    "        evidence = {c: int(df_disc.iloc[i][c]) for c in feature_cols}\n",
    "        q = infer.query(variables=[\"y\"], evidence=evidence, show_progress=False)\n",
    "        # q.values aligned with state order 0..K-1\n",
    "        vals = q.values\n",
    "        # guard: sometimes pgmpy returns float64\n",
    "        probas[i, :] = vals[:len(y_states)]\n",
    "    return probas\n",
    "\n",
    "proba = predict_proba_bn(infer, test_disc, feature_cols)\n",
    "y_pred = proba.argmax(axis=1)\n",
    "\n",
    "\n",
    "print(\"\\n===== BN Naive Bayes (3-class) : Classification Report =====\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "print(\"\\n===== BN Naive Bayes (3-class) : Confusion Matrix =====\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Threshold-level AUCs (ordinal/cumulative): P(y>=1), P(y>=2)\n",
    "p_ge_1 = proba[:, 1] + proba[:, 2]\n",
    "p_ge_2 = proba[:, 2]\n",
    "\n",
    "y_ge_1 = (y_test.values >= 1).astype(int)\n",
    "y_ge_2 = (y_test.values >= 2).astype(int)\n",
    "\n",
    "print(\"\\n===== Threshold-level ROC AUC (BN) =====\")\n",
    "print(f\"AUC P(y>=1): {roc_auc_score(y_ge_1, p_ge_1):.4f}\")\n",
    "print(f\"AUC P(y>=2): {roc_auc_score(y_ge_2, p_ge_2):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6618ddd9-2f31-4dcf-b954-b6146c32be9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
